{"name":"LightMem","repository_url":"https://github.com/zjunlp/LightMem","stars":564,"primary_language":"Python","description":"轻量级高效记忆管理框架，采用LLMlingua-2压缩技术实现98%token减少和117倍更低token消耗，适用于LLM和AI智能体（ICLR 2026）","last_updated":"2026-01-26","paper":{"exists":true,"title":"LightMem: Lightweight and Efficient Memory Management","venue":"ICLR","year":2026,"url":"https://arxiv.org/abs/unknown"},"benchmarks":{"locomo":{"score":0,"details":"Up to 10.9% accuracy gains, 117× less tokens, 159× fewer API calls, 12× faster runtime"},"longmemeval":{"score":0,"details":"Complete LongMemEval support with reproduction scripts"}},"tech_stack":{"storage":["Lightweight Storage","Modular Components"],"frameworks":["Python","MCP Server","vLLM"],"languages":["Python"],"embedding_models":["OpenAI","DeepSeek","Ollama"]},"cloud_needs":{"storage":{"types":["Minimal Storage","Resource-efficient"],"requirements":["Fast response","Simple API"]},"compute":{"embedding":true,"gpu_needed":false,"estimated_requirements":"Minimal compute, 2-8 vCPUs"},"deployment":{"complexity":4,"containerized":true,"orchestration":["Docker","Lightweight deployment"]}},"categories":{"tech_approach":["Lightweight","Efficient","ICLR 2026"],"use_case":["Resource-constrained","Fast Response"]},"innovations":{"key_features":["LLMlingua-2压缩技术实现98% token减少","117倍更低token消耗，159倍更少API调用","准确率提升高达10.9%，运行速度提升12倍","模块化设计，支持多种LLM提供商(OpenAI/DeepSeek/Ollama)","MCP Server集成，Claude等工具原生支持"],"improvements":["相比传统方案节省98%的token成本","在LoCoMo benchmark上达到最优性能","完整支持LongMemEval基准测试","比传统记忆系统快12倍运行速度","支持vLLM本地部署降低成本"],"user_value":["大幅降低LLM API调用成本（中型部署可节省$173,904/月）","提升智能体响应速度和准确率","资源受限环境下的最佳选择","开箱即用的Docker部署方案","ICLR 2026顶会论文支撑的技术可靠性"]},"use_cases":{"scenarios":["成本敏感的AI应用，需要降低LLM API费用","资源受限环境，无法承担大规模token消耗","高频调用场景，需要快速响应","企业级智能体，需要在性能和成本间平衡","Claude MCP集成应用"],"companies":["浙江大学知识引擎实验室(开发团队)","成本优化型AI创业公司","中小企业AI应用场景","教育科研机构","Claude开发者生态"]}}